{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93127276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, max, min, to_date\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "# from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import os\n",
    "import shapefile\n",
    "from pmdarima.arima import auto_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272443ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/01 14:50:12 WARN Utils: Your hostname, cyber-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/11/01 14:50:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/01 14:50:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"FlubyRegion\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22a113ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"flu_region\"\n",
    "data_dir = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "28cdb2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+----------+---------+-------------------+-----------+--------------------------+\n",
      "|   season|date_code|weekending|    region|Total_ILI|Total_Patients_Seen|Percent_ILI|Number_Providers_Reporting|\n",
      "+---------+---------+----------+----------+---------+-------------------+-----------+--------------------------+\n",
      "|2001-2002|   200140| 10/6/2001|  01675839|        3|                135|       2.22|                         2|\n",
      "|2001-2002|   200140| 10/6/2001|  01675903|        3|                135|       2.22|                         2|\n",
      "|2001-2002|   200140| 10/6/2001|  00277285|        3|                135|       2.22|                         2|\n",
      "|2001-2002|   200140| 10/6/2001|  00277292|        3|                135|       2.22|                         2|\n",
      "|2001-2002|   200140| 10/6/2001|  00277302|        3|                135|       2.22|                         2|\n",
      "|2001-2002|   200140| 10/6/2001|  00277305|        3|                135|       2.22|                         2|\n",
      "|2001-2002|   200140| 10/6/2001|  00277307|        3|                135|       2.22|                         2|\n",
      "|2001-2002|   200140| 10/6/2001|  00277308|        3|                135|       2.22|                         2|\n",
      "|2001-2002|   200140| 10/6/2001|  00277312|        3|                135|       2.22|                         2|\n",
      "|2001-2002|   200140| 10/6/2001|  01657246|        3|                135|       2.22|                         2|\n",
      "|2001-2002|   200140| 10/6/2001|California|       25|               1211|       2.06|                        14|\n",
      "|2001-2002|   200140| 10/6/2001|  01675885|        0|                  0|       NULL|                         0|\n",
      "|2001-2002|   200140| 10/6/2001|  00277274|        0|                  0|       NULL|                         0|\n",
      "|2001-2002|   200140| 10/6/2001|  01804637|        0|                  0|       NULL|                         0|\n",
      "|2001-2002|   200140| 10/6/2001|  00277280|        0|                  0|       NULL|                         0|\n",
      "|2001-2002|   200140| 10/6/2001|  00277284|        0|                  0|       NULL|                         0|\n",
      "|2001-2002|   200140| 10/6/2001|  00277286|        0|                  0|       NULL|                         0|\n",
      "|2001-2002|   200140| 10/6/2001|  00277288|        0|                  0|       NULL|                         0|\n",
      "|2001-2002|   200140| 10/6/2001|  00277290|        0|                  0|       NULL|                         0|\n",
      "|2001-2002|   200140| 10/6/2001|  00277291|        0|                  0|       NULL|                         0|\n",
      "+---------+---------+----------+----------+---------+-------------------+-----------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Divide Flu-by-Region into Flu-by-County\n",
    "filecheck1 = f\"{root_dir}/flu_by_county.csv\"\n",
    "if not os.path.isfile(filecheck1):\n",
    "    fbc_dict = {\n",
    "    \"Bay Area\": [\"01675839\" , \"01675903\", \"00277285\", \"00277292\", \"00277302\", \"00277305\" , \"00277307\",  \"00277308\" , \"00277312\", \"01657246\"],\n",
    "    \"Central\": [\"01675885\", \"00277274\", \"01804637\", \"00277280\", \"00277284\", \"00277286\", \"00277288\", \"00277290\", \"00277291\", \"00277299\", \"00277303\", \"00277314\", \"00277318\", \"00277319\"],\n",
    "    \"Lower Southern\" : [\"00277277\", \"00277294\", \"00277297\", \"00277300\", \"00277301\"],\n",
    "    \"Northern\" : [\"01675840\", \"01675841\", \"01675842\", \"01675902\", \"01682074\", \"00277273\", \"00277275\", \"01681908\", \"00277281\", \"01693324\", \"00277287\", \"00277289\", \"01682927\", \"00277295\", \"00277296\",  \"00277298\", \"01682610\", \"00277310\", \"00277311\", \"00277315\", \"01692767\", \"00277317\", \"00277321\", \"00277322\"],\n",
    "    \"Upper Southern\" : [\"02054176\", \"00277283\", \"00277304\", \"00277306\", \"00277320\"]\n",
    "    }\n",
    "\n",
    "    df = pd.read_csv(f\"{data_dir}/flu-ili-byregion-fluseason.csv\")\n",
    "    # Create an empty list to store new dataframes\n",
    "    new_dfs = []\n",
    "\n",
    "    # Loop through each row in the original DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        x_value = row[\"region\"]\n",
    "        values_to_replace = fbc_dict.get(x_value, [x_value])\n",
    "\n",
    "        # Create new dataframes for each value in values_to_replace\n",
    "        for new_x_value in values_to_replace:\n",
    "            new_df = row.copy().to_frame().T  # Create a new DataFrame with one row\n",
    "            new_df[\"region\"] = new_x_value\n",
    "            new_dfs.append(new_df)\n",
    "\n",
    "    # Concatenate the list of dataframes into a single dataframe\n",
    "    new_df = pd.concat(new_dfs, ignore_index=True)\n",
    "\n",
    "    # Write the modified DataFrame to a CSV file\n",
    "    new_df.to_csv(f\"{root_dir}/flu_by_county.csv\", index=False)\n",
    "\n",
    "df = spark.read.csv(f\"{root_dir}/flu_by_county.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "865f31b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Divide data into separate files by date\n",
    "filecheck2 = f\"{root_dir}/flu_by_dates\"\n",
    "if not os.listdir(filecheck2):\n",
    "    # Create a Spark session\n",
    "\n",
    "    # Read your data into a DataFrame (replace \"input_file.csv\" with your file)\n",
    "    df = spark.read.csv(f\"{root_dir}/flu_by_county.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    # Define the column to group by\n",
    "    group_column = \"date_code\"  # Replace with your actual column name\n",
    "\n",
    "    # Get distinct values in the group column\n",
    "    distinct_values = [str(row[group_column]) for row in df.select(group_column).distinct().collect()]\n",
    "\n",
    "    # Group by the column and write each group to a separate file\n",
    "    for value in distinct_values:\n",
    "        group_df = df.filter(col(group_column) == value)\n",
    "        output_path = f\"{filecheck2}/{value}.csv\"  # Replace with your desired output directory\n",
    "        group_df.write.csv(output_path, header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "788fb9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Heatmaps\n",
    "maps_dir = f\"{root_dir}/maps\"\n",
    "\n",
    "# Check if the directory is empty\n",
    "if not os.listdir(maps_dir):\n",
    "    ca_map = gpd.read_file(\"CA_Counties/CA_Counties_TIGER2016.shp\")\n",
    "    \n",
    "    # List all subdirectories in the root directory\n",
    "    sub_dir = [d for d in os.listdir(filecheck2) if os.path.isdir(os.path.join(filecheck2, d))]\n",
    "\n",
    "    # Iterate through the subdirectories and read CSV files in each subdirectory\n",
    "    for sub in sub_dir:\n",
    "        subdirectory_path = os.path.join(filecheck2, sub)\n",
    "        csv_files = [f for f in os.listdir(subdirectory_path) if f.endswith('.csv')]\n",
    "\n",
    "        for csv_file in csv_files:\n",
    "            csv_file_path = os.path.join(subdirectory_path, csv_file)\n",
    "            df = pd.read_csv(csv_file_path)\n",
    "            \n",
    "            # Calculate the maximum and minimum values in the \"Total_ILI\" column\n",
    "            max_value = df[\"Total_ILI\"].max()\n",
    "            min_value = df[\"Total_ILI\"].min()\n",
    "            \n",
    "            #Merge files\n",
    "            mapf = \"COUNTYNS\"\n",
    "            statf = \"region\"\n",
    "            map_and_stats = ca_map.merge(df, left_on=mapf, right_on=statf)\n",
    "            \n",
    "            #plot map\n",
    "            fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "            plt.xticks(rotation=90)\n",
    "            map_and_stats.plot(column=\"Total_ILI\", cmap=\"Reds\", linewidth=0.4, ax=ax, edgecolor=\".4\")\n",
    "            title_column = \"weekending\"\n",
    "            title_value = df[title_column].iloc[0]\n",
    "            plt.title(f\"Total IRI for Week of {title_value}\")\n",
    "            bar_info = plt.cm.ScalarMappable(cmap=\"Reds\", norm=plt.Normalize(vmin=min_value, vmax=max_value))\n",
    "            bar_info._A = []\n",
    "            cbar = fig.colorbar(bar_info)\n",
    "            \n",
    "            # Save the map as an image file in the \"maps\" directory\n",
    "            output_file = os.path.join(maps_dir, os.path.splitext(csv_file)[0] + \".png\")\n",
    "            plt.savefig(output_file, dpi=300)  # Change the dpi and format as needed\n",
    "            \n",
    "            # Close the figure to release memory\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ac3420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Predictive map\n",
    "filecheck3 = f\"{root_dir}/flu_by_county.csv\"\n",
    "if os.path.isfile(filecheck3):\n",
    "    # Read the CSV file into a PySpark DataFrame\n",
    "    df = spark.read.csv(f\"{root_dir}/flu_by_county.csv\", header=True, inferSchema=True)\n",
    "    #df.show()\n",
    "    # Select the relevant columns\n",
    "    df = df.select(\"weekending\", \"Total_ILI\")\n",
    "\n",
    "    # Convert the PySpark DataFrame to a Pandas DataFrame\n",
    "    pandas_df = df.toPandas()\n",
    "\n",
    "    # Convert the \"weekending\" column to a datetime format\n",
    "    pandas_df[\"weekending\"] = pd.to_datetime(pandas_df[\"weekending\"])\n",
    "\n",
    "    # Create the Matplotlib plot\n",
    "    plt.plot(pandas_df[\"weekending\"], pandas_df[\"Total_ILI\"])\n",
    "\n",
    "    # Set labels for the x and y axes\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Total_ILI\")\n",
    "    \n",
    "    df = df.select(\"weekending\", \"Total_ILI\")\n",
    "    df = df.withColumnRenamed(\"Total_ILI\", \"target\")\n",
    "\n",
    "    # Collect the data to the driver for processing with Pyramid ARIMA\n",
    "    data = df.select(\"weekending\", \"target\").orderBy(\"weekending\").toPandas()\n",
    "\n",
    "    # Train the ARIMA model using the Pyramid ARIMA library\n",
    "    # You can adjust the ARIMA parameters as needed\n",
    "    stepwise_model = auto_arima(\n",
    "        data[\"target\"],\n",
    "        seasonal=True,\n",
    "        m=12,  # Define the seasonality (e.g., 12 for monthly data)\n",
    "        stepwise=True\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    # You can replace this with your actual future data\n",
    "    future_data = df.select(\"weekending\").distinct().toPandas()\n",
    "    predictions, conf_int = stepwise_model.predict(n_periods=future_data.shape[0], return_conf_int=True)\n",
    "\n",
    "    # Combine predictions with future data\n",
    "    future_data[\"predicted_ILI\"] = predictions\n",
    "\n",
    "    # Display or save the predictions\n",
    "    # You can save the predictions as needed or show them\n",
    "    future_data.show()\n",
    "\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e18099",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a23992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
